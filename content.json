{"pages":[],"posts":[{"title":"VGG paper takenote","text":"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITIONTLDR: The authors proposed using the very small filter in the convolution layers make the networks more depths and worked better. Beside that, by using the small filter (3x3), the total of params is decreasing. Key points Replace the large receptive filter in the first layer (5x5, 7x7, 11x11 with big stride) by small filter (3x3). Stack 2 layers 3x3 convolution (without activation) is like 5x5, or 3 layer 3x3 like 7x7, but it make model has less params =&gt; efficient. The authors use 1x1 convolution is the way to increase the non linear of decision funtion without afecting the receptive field of convolution layers. In the paper, the 1x1 is essentially a linear projection onto the space of same dimensionality. The authors conjecture that the model required less epoch to converge, due to implicit regularisation imposed by greater depth and smaller conv filter sizes.","link":"/2020/06/05/VGG-paper-takenote/"},{"title":"MobileNetv1-paper-takenote","text":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision ApplicationsTL,DR: The authors presented a class of efficient models called MobileNets for mobile and embedded vision applications. Those models are based on a streamlined architecture that uses depth wise separable convolutions to build the light weight models. Beside that, the authors introduced two simple hyperparameters that controle the trade off between latency and accuracy ($\\alpha$ and $p$). Key points: Depth wise separable convolution is the convolution that separates the operation into 2 operation. First, convolution for each depth layer. After that, we use 1x1 convolution to combine the input in the depth dimension. The advantage of the depth wise separable convolution is that do not require much more computiation cost, reduce time and size of model. The authors also representd two hyperparams for control efficiently the trade off between latancy and accuracy of model, $\\alpha$ and $p$. $\\alpha$ control the size of the model, by controling the number of the filters in each layer of the model. $p$ control the resolution of the input.","link":"/2020/06/05/MobileNetv1-paper-takenote/"},{"title":"Resnet paper-takenote","text":"Deep Residual Learning for Image Recognition arXiv:1512.03385v1TL,DR: The paper proposed the new architecture for building block called Residual block. Residual block is showed that it can make the model become more deeper and efficient. It resolved the problem that when the model become deep, the loss increase. Key notes: Residual connections is y = F(x, W) + x, where F is representation of 2 or 3 layer convolution. The authors suggest it should not be 1 layer convolution. For the correction of demension, we have the identity shortcut y = F(x, W) + x or projection shortcut y = F(x, W) + W(x). Because the projection shortcut not show the much better than any shortcut, the authors did not use them to reduce the cost of computing. The bottleneck design are showed as a way to make the model efficient in compute. They use 1x1 convolution to reduce the dimensional on depth and next increase that. In deeper Resnet model such that Resnet 152, the authors use Bottleneck block for reduce computational cost.","link":"/2020/06/07/Resnet-paper-takenote/"}],"tags":[{"name":"Machine Learning, Deep Learning, Computer Vison, paper takenote","slug":"Machine-Learning-Deep-Learning-Computer-Vison-paper-takenote","link":"/tags/Machine-Learning-Deep-Learning-Computer-Vison-paper-takenote/"}],"categories":[{"name":"Computer Vision","slug":"Computer-Vision","link":"/categories/Computer-Vision/"}]}